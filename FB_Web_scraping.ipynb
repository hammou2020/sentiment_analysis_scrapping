{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request token\n",
      "fresh access token: 1279153855620167|BPgJGoHxdsQI7NPUVNdPwFAlLHE\n",
      "\n",
      "Scraping's posts phase start...\n",
      "\n",
      "HTTP Error 400: Bad Request\n",
      "Error for URL https://graph.facebook.com/v2.11/laanews/posts/1311167095711069?__tn__=H-R1279153855620167|BPgJGoHxdsQI7NPUVNdPwFAlLHE&limit=25: 2020-05-03 21:54:19.580999\n",
      "Retrying.\n",
      "HTTP Error 400: Bad Request\n",
      "Error for URL https://graph.facebook.com/v2.11/laanews/posts/1311167095711069?__tn__=H-R1279153855620167|BPgJGoHxdsQI7NPUVNdPwFAlLHE&limit=25: 2020-05-03 21:54:25.001797\n",
      "Retrying.\n",
      "HTTP Error 400: Bad Request\n",
      "Error for URL https://graph.facebook.com/v2.11/laanews/posts/1311167095711069?__tn__=H-R1279153855620167|BPgJGoHxdsQI7NPUVNdPwFAlLHE&limit=25: 2020-05-03 21:54:30.428672\n",
      "Retrying.\n",
      "HTTP Error 400: Bad Request\n",
      "Error for URL https://graph.facebook.com/v2.11/laanews/posts/1311167095711069?__tn__=H-R1279153855620167|BPgJGoHxdsQI7NPUVNdPwFAlLHE&limit=25: 2020-05-03 21:54:35.747336\n",
      "Retrying.\n",
      "HTTP Error 400: Bad Request\n",
      "Error for URL https://graph.facebook.com/v2.11/laanews/posts/1311167095711069?__tn__=H-R1279153855620167|BPgJGoHxdsQI7NPUVNdPwFAlLHE&limit=25: 2020-05-03 21:54:41.083443\n",
      "Retrying.\n",
      "HTTP Error 400: Bad Request\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a0fd22dca69a>\u001b[0m in \u001b[0;36mrequest_until_succeed\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    640\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 641\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 400: Bad Request",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a0fd22dca69a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;31m# 1.next_value = i post che vanno da 26 a 50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;31m# 2.next_value = i post che vanno da 51 a 75 etc...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m     \u001b[0mscrape_all_posts_in_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_page\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nScraping's posts phase stop...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-a0fd22dca69a>\u001b[0m in \u001b[0;36mscrape_all_posts_in_page\u001b[0;34m(url, num_page)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscrape_all_posts_in_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_page\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0mjson_downloaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest_until_succeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_downloaded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Json(casted in dictionary) data from our request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mnext_post\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_downloaded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'paging'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# this are used for take 'next' element in the dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-a0fd22dca69a>\u001b[0m in \u001b[0;36mrequest_until_succeed\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error for URL {}: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 17/01/2018\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import pprint\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# set since 3000 recursions, for post with >= 25000 comments\n",
    "sys.setrecursionlimit(3000)\n",
    "\n",
    "try:\n",
    "    from urllib.request import urlopen, Request\n",
    "except ImportError:\n",
    "    from urllib2 import urlopen, Request\n",
    "\n",
    "app_id = \"1279153855620167\"\n",
    "app_secret = \"ac4cb31ea1286f1ee4bb2ed75233e7ca\"  # DO NOT SHARE WITH ANYONE!\n",
    "# !!GOOD BOTH FOR PAGE AND FOR GROUPS!!\n",
    "page_id = \"laanews\" \n",
    "#page: https://www.facebook.com/laanews/posts/1311167095711069?__tn__=H-R\n",
    "num_post = 0\n",
    "num_comments = 0\n",
    "num_comm_per_page = 25\n",
    "\n",
    "def add_num_post(value):\n",
    "    global num_post\n",
    "    num_post = num_post + value\n",
    "\n",
    "def add_num_comments(value):\n",
    "    global num_comments\n",
    "    num_comments = num_comments + value\n",
    "\n",
    "def request_until_succeed(url):\n",
    "    req = Request(url)\n",
    "    success = False\n",
    "    while success is False:\n",
    "        try:\n",
    "            response = urlopen(req)\n",
    "            if response.getcode() == 200:\n",
    "                success = True\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(5)\n",
    "\n",
    "            print(\"Error for URL {}: {}\".format(url, datetime.datetime.now()))\n",
    "            print(\"Retrying.\")\n",
    "\n",
    "    return response.read()\n",
    "\n",
    "\n",
    "def writeFile(path, name, text):\n",
    "    write_file = open(path + name,\"w+\")\n",
    "    write_file.write(text)\n",
    "    write_file.close()\n",
    "\n",
    "# The function for taking comments is recursive. Comments are taken 25 at a time.\n",
    "# There is a maximum number of recursion for the python interpreter (= 1000).\n",
    "# If the post has a number of comments > 25000 (25000/25 = 1000), our interpreter crash.\n",
    "# This function is useful for dynamically increase the maximum number of recursions possible by the Python interpreter\n",
    "def set_recursion_limit(total_comments):\n",
    "    for key, value in total_comments.iteritems() :\n",
    "        if key==\"total_count\":\n",
    "            sys.setrecursionlimit( value/num_comm_per_page )\n",
    "\n",
    "####################################################################\n",
    "#                       SCRAPING POSTs                             #\n",
    "####################################################################\n",
    "\n",
    "# first request will be of the type:\n",
    "# https://graph.facebook.com/v2.11/page_id/posts?access_token=....\n",
    "# then, will be gather all the values next in the json file in order to do at the next request\n",
    "def scrape_first_posts_in_page(page_id, access_token):\n",
    "    base = \"https://graph.facebook.com/v2.11/\"\n",
    "    parameters = \"&access_token={}\".format(access_token)\n",
    "    fields = \"?fields=posts\"\n",
    "    num_page = 1\n",
    "\n",
    "    url = base + page_id + fields + parameters\n",
    "    #print(url)\n",
    "    print(\"\\n scraping posts in page: \" + str(num_page))\n",
    "    json_downloaded = request_until_succeed(url)\n",
    "    data = json.loads(json_downloaded)['posts']['data'] # Json(casted in list) data from our request\n",
    "    next_post = json.loads(json_downloaded)['posts']['paging'] # this are used for take 'next' element in the dictionary\n",
    "    for key, value in next_post.iteritems() :\n",
    "        if key==\"next\":\n",
    "            next_value = value\n",
    "\n",
    "    writeFile(\"./posts/\", str(num_page) + \".next_value.txt\", next_value)\n",
    "    print(\"\\n writing \"+ str(num_page) +\" next_value\")\n",
    "\n",
    "    loops_for_scraping_comments(num_page, data)\n",
    "\n",
    "    scrape_all_posts_in_page(next_value, num_page + 1)\n",
    "\n",
    "def scrape_all_posts_in_page(url, num_page):\n",
    "    json_downloaded = request_until_succeed(url)\n",
    "    data = json.loads(json_downloaded)['data'] # Json(casted in dictionary) data from our request\n",
    "    next_post = json.loads(json_downloaded)['paging'] # this are used for take 'next' element in the dictionary\n",
    "\n",
    "    #pp = pprint.PrettyPrinter(indent=2)\n",
    "    #pp.pprint(next_post)\n",
    "    for key, value in next_post.iteritems() :\n",
    "        if key==\"next\":\n",
    "            next_value = value\n",
    "            writeFile(\"./posts/\", str(num_page) + \".next_value\", value)\n",
    "            print(\"\\n writing \"+ str(num_page) +\" next_value\")\n",
    "\n",
    "    print(\"\\n scraping posts in page: \" + str(num_page))\n",
    "\n",
    "    loops_for_scraping_comments(num_page, data)\n",
    "\n",
    "    scrape_all_posts_in_page(next_value, num_page + 1)\n",
    "\n",
    "####################################################################\n",
    "#                       SCRAPE POST'S COMMENTS                     #\n",
    "####################################################################\n",
    "\n",
    "# function for scrape single post's comments\n",
    "def loops_for_scraping_comments(num_page, data):\n",
    "    # retrieve data, message and id (useful for querying the comments)\n",
    "    extension=\".json\"\n",
    "    i = 0\n",
    "    #print(len(data))\n",
    "    #count num_post\n",
    "    add_num_post(len(data))\n",
    "\n",
    "    while (i < len(data)):\n",
    "        print(\"\\n   scraping post \" + str(i + 1)  + \" in page: \" + str(num_page))\n",
    "\n",
    "        created_time = data[i]['created_time']\n",
    "\n",
    "        # use get method over the dictionary because the comment couldn't exist and Facebook doesn't generate the corresponding item in the Json file\n",
    "        if data[i].get('message') is None:\n",
    "            message = \"\"\n",
    "        else:\n",
    "            message = data[i].get('message').encode(\"utf-8\")\n",
    "\n",
    "        id_post = data[i]['id']\n",
    "        scrape_starttime = datetime.datetime.now()\n",
    "        comments = scrape_first_comments_from_post_id(id_post, access_token)\n",
    "        print(\"   Done! Comment Processed in {}\".format(datetime.datetime.now() - scrape_starttime))\n",
    "\n",
    "        name_file =  str(created_time).replace(':','.') + \"page_\" + str(num_page) + \"_posts\" + str(i + 1)\n",
    "        writeFile(name_file + extension, str(created_time) + \"\\n\\n\" + str(message) + \"\\n\\n\" + str(id_post) + \"\\n\\n\" + str(comments) + \"\\n\\n\")\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "def scrape_first_comments_from_post_id(post_id, access_token):\n",
    "    # with filter=stream should also gather the aswers to comments, but it seems doesn't work\n",
    "    # https://graph.facebook.com/v2.11/post_id/comments?filter=stream&summary=true&access_token=2081983152047773|cUqdwRV6VnEZBTwAwmv5wdBQEBw\n",
    "    base = \"https://graph.facebook.com/v2.11/\"\n",
    "    parameters = \"&access_token={}\".format(access_token)\n",
    "    fields = \"/comments?filter=stream&summary=true\"\n",
    "\n",
    "    scr_data = []\n",
    "    url= base + post_id + fields + parameters\n",
    "    json_downloaded = request_until_succeed(url)\n",
    "\n",
    "    data = json.loads(json_downloaded)['data']\n",
    "\n",
    "    if json.loads(json_downloaded).get('paging') is None:\n",
    "        next_post = {}\n",
    "    else:\n",
    "        next_post = json.loads(json_downloaded).get('paging')\n",
    "\n",
    "    total_comments = json.loads(json_downloaded)['summary']\n",
    "\n",
    "    #set_recursion_limit(total_comments)\n",
    "\n",
    "    comment_count = 0\n",
    "    for count in data:\n",
    "        comment_count += 1\n",
    "\n",
    "    #count comments\n",
    "    add_num_comments(comment_count)\n",
    "\n",
    "    #search next post url\n",
    "    for key, value in next_post.iteritems() :\n",
    "        if key==\"next\":\n",
    "            scr_data = scrape_all_comments_from_post_id(value)\n",
    "\n",
    "    return data + scr_data\n",
    "\n",
    "def scrape_all_comments_from_post_id(url):\n",
    "    scr_data = []\n",
    "    json_downloaded = request_until_succeed(url)\n",
    "\n",
    "    data = json.loads(json_downloaded)['data'] # Json(casted in dictionary) data from our request\n",
    "    next_post = json.loads(json_downloaded)['paging'] # this are used for take 'next' element in the dictionary\n",
    "\n",
    "    comment_count = 0\n",
    "    for count in data:\n",
    "        comment_count += 1\n",
    "\n",
    "    #count comments\n",
    "    add_num_comments(comment_count)\n",
    "\n",
    "    for key, value in next_post.iteritems() :\n",
    "        if key==\"next\":\n",
    "            scr_data = scrape_all_comments_from_post_id(value)\n",
    "\n",
    "    return data + scr_data\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    filename_n_v = []\n",
    "\n",
    "    for filename in os.listdir('./'):\n",
    "        if filename.endswith(\".next_value\"):\n",
    "            filename_n_v.append(filename)\n",
    "\n",
    "\n",
    "    num_page = 1\n",
    "\n",
    "\n",
    "    print(\"Request token\")\n",
    "    #new access_token\n",
    "    access_token_request = request_until_succeed(\"https://graph.facebook.com/v2.11/oauth/access_token?client_id=\" + app_id + \"&client_secret=\" + app_secret + \"&grant_type=client_credentials\")\n",
    "    access_token = json.loads(access_token_request)[\"access_token\"] #!type unicode!\n",
    "    print(\"fresh access token: \" + access_token + \"\\n\")\n",
    "\n",
    "    url=\"https://graph.facebook.com/v2.11/\"+ page_id +\"/posts/1311167095711069?__tn__=H-R\" + access_token + \"&limit=25\"\n",
    "\n",
    "    scrape_starttime = datetime.datetime.now()\n",
    "    # start scraping's post phase\n",
    "    print(\"Scraping's posts phase start...\\n\")\n",
    "\n",
    "    # start from begin\n",
    "    # scrape_first_posts_in_page(page_id, access_token)\n",
    "\n",
    "    # start from a block's posts\n",
    "    # files with next_value extension are:\n",
    "    # 1.next_value = i post che vanno da 26 a 50\n",
    "    # 2.next_value = i post che vanno da 51 a 75 etc...\n",
    "    scrape_all_posts_in_page(url, num_page)\n",
    "\n",
    "    print(\"\\nScraping's posts phase stop...\")\n",
    "\n",
    "    print(\"\\nDone!\\n{} Comments Processed in {}\".format(\n",
    "            num_processed, datetime.datetime.now() - scrape_starttime))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
